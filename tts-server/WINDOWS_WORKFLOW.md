# Windows Native Workflow - TTS Server

## ðŸ“ Your Working Directory

```
C:\Users\dsm27\little_timmy\tts-server\
```

This is your **main working directory** for the TTS server, connected to GitHub.

---

## ðŸš€ Quick Start

### **Start TTS Server**
Double-click: `START_TTS_SERVER.bat`

This will:
1. Activate the virtual environment
2. Start the TTS server on port 5051
3. Enable CUDA GPU acceleration
4. Wait for text-to-speech requests

---

## ðŸ“ Making Changes

### 1. Edit Files
Work directly in this directory:
- `timmy_speaks_cuda.py` - Main TTS server code
- `config.py` - Configuration settings
- `requirements.txt` - Python dependencies
- Utility scripts (`list_devices.py`, `send_speech.py`, etc.)

### 2. Commit Changes
```bash
cd C:\Users\dsm27\little_timmy
git add tts-server/
git commit -m "Your commit message"
git push origin main
```

### 3. Pull Latest Changes
```bash
cd C:\Users\dsm27\little_timmy
git pull origin main
```

---

## ðŸ”„ Setup from Scratch

If you're setting up on a new machine or fresh install:

### 1. Clone Repository
```bash
cd C:\Users\dsm27
git clone https://github.com/dan-gearscodeandfire/little_timmy.git
cd little_timmy\tts-server
```

### 2. Create Virtual Environment
```bash
python -m venv C:\Users\dsm27\piper\.venv
```

### 3. Activate Virtual Environment
```bash
C:\Users\dsm27\piper\.venv\Scripts\activate.bat
```

### 4. Install Dependencies
```bash
pip install -r requirements.txt
```

### 5. Install CUDA Support (GPU Acceleration)
```bash
pip install onnxruntime-gpu
```

This will automatically install:
- nvidia-cudnn-cu12
- nvidia-cuda-runtime-cu12
- nvidia-cublas-cu12
- Other CUDA toolkit components

### 6. Get Voice Models

Models are **not in the repository** (too large).

**Option A: Use Existing Models**
If you have models already:
```bash
# Copy from your existing piper installation
mkdir models
copy C:\Users\dsm27\piper\models\*.onnx models\
copy C:\Users\dsm27\piper\models\*.json models\
```

**Option B: Download Models**
Download Piper voice models from:
- Official Piper TTS sources
- Place `*.onnx` and `*.onnx.json` files in `models/` directory

---

## ðŸ“‚ Directory Structure

```
C:\Users\dsm27\
â”œâ”€â”€ little_timmy\                 â† Git repository root
â”‚   â”œâ”€â”€ tts-server\               â† YOUR WORKING DIRECTORY
â”‚   â”‚   â”œâ”€â”€ START_TTS_SERVER.bat  â† Double-click to run
â”‚   â”‚   â”œâ”€â”€ timmy_speaks_cuda.py  â† Main server
â”‚   â”‚   â”œâ”€â”€ config.py             â† Configuration
â”‚   â”‚   â”œâ”€â”€ requirements.txt      â† Dependencies
â”‚   â”‚   â”œâ”€â”€ send_speech.py        â† Test utility
â”‚   â”‚   â”œâ”€â”€ list_devices.py       â† Audio device utility
â”‚   â”‚   â”œâ”€â”€ convert_to_fp16.py    â† Model conversion utility
â”‚   â”‚   â”œâ”€â”€ models\               â† Voice models (not in git)
â”‚   â”‚   â”‚   â”œâ”€â”€ skeletor_v1.onnx
â”‚   â”‚   â”‚   â””â”€â”€ skeletor_v1.onnx.json
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ WINDOWS_WORKFLOW.md   â† This file
â”‚   â”œâ”€â”€ stt-server-v17\           â† STT server
â”‚   â”œâ”€â”€ v34\                      â† LLM preprocessor
â”‚   â””â”€â”€ .git\                     â† Git repository data
â””â”€â”€ piper\
    â””â”€â”€ .venv\                    â† Virtual environment (shared)
```

---

## ðŸŒ Network Configuration

- **TTS Server (This)**: http://localhost:5051 or http://192.168.1.154:5051
- **STT Server**: http://localhost:8888
- **LLM Preprocessor**: http://localhost:5000 (WSL)
- **ESP32 Display**: https://192.168.1.110:8080

---

## âš™ï¸ Virtual Environment

The TTS server uses a **dedicated virtual environment**:
```
C:\Users\dsm27\piper\.venv
```

### Manual Activation
If you need to manually activate:
```bash
C:\Users\dsm27\piper\.venv\Scripts\activate.bat
```

### Why Separate from STT?
- Different dependencies (piper-tts vs faster-whisper)
- Avoids version conflicts
- Each can be updated independently
- Cleaner dependency management

---

## ðŸŽ¯ Configuration

### Edit Settings
Open `config.py` and modify:

```python
# STT Server endpoint
HEARING_SERVER_URL = "http://localhost:8888"

# ESP32 LCD display
SKULL_EYE_ENDPOINT = "https://192.168.1.110:8080/esp32/write"

# Server settings
TTS_PORT = 5051
DEFAULT_SPEECH_SPEED = 0.6  # Lower = faster
```

### Command-Line Overrides
You can also override settings when starting:
```bash
python timmy_speaks_cuda.py --port 5052 --length-scale 0.8
```

---

## ðŸ§ª Testing

### Test Audio Output
```bash
python list_devices.py
```

### Test TTS Server
```bash
# Start the server first (in one terminal)
START_TTS_SERVER.bat

# Then in another terminal, send test speech
python send_speech.py
```

### Test via HTTP
```bash
curl "http://localhost:5051/?text=Hello%20world"
```

---

## ðŸ”§ Troubleshooting

### Server Won't Start
1. Check virtual environment exists:
   ```bash
   dir C:\Users\dsm27\piper\.venv
   ```
2. Verify dependencies installed:
   ```bash
   C:\Users\dsm27\piper\.venv\Scripts\python.exe -m pip list
   ```

### CUDA Not Working
1. Check if CUDA provider is available:
   ```bash
   C:\Users\dsm27\piper\.venv\Scripts\python.exe
   >>> import onnxruntime as ort
   >>> ort.get_available_providers()
   # Should include 'CUDAExecutionProvider'
   ```

2. Update NVIDIA drivers
3. Reinstall onnxruntime-gpu:
   ```bash
   pip uninstall onnxruntime-gpu
   pip install onnxruntime-gpu
   ```

### No Audio Output
1. List audio devices:
   ```bash
   python list_devices.py
   ```
2. Check Windows sound settings
3. Verify default output device is correct

### Models Not Found
1. Check models directory exists and contains:
   - `skeletor_v1.onnx`
   - `skeletor_v1.onnx.json`
2. Or specify model path:
   ```bash
   python timmy_speaks_cuda.py -m "C:\path\to\your\model.onnx"
   ```

---

## ðŸ“Š Performance

### Expected Performance (RTX 3060)
- Synthesis: 2-4x real-time
- Latency: <500ms for short phrases
- GPU Memory: ~1-2GB during synthesis

### Monitor Performance
```bash
# Check last synthesis metrics
curl http://localhost:5051/metrics
```

---

## ðŸŽ“ Benefits of This Setup

âœ… **Native Windows Performance** - Direct GPU access, no WSL overhead  
âœ… **Git Integrated** - Push/pull changes to GitHub easily  
âœ… **Isolated Dependencies** - Separate venv prevents conflicts  
âœ… **Well Documented** - Clear setup and troubleshooting guides  
âœ… **Production Ready** - CUDA acceleration for fast synthesis

---

## ðŸ“Œ Related Documentation

- **Main README**: `README.md` - Full API documentation
- **STT Server**: `../stt-server-v17/WINDOWS_WORKFLOW.md`
- **LLM Preprocessor**: `../v34/` (runs in WSL)

